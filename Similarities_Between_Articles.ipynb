{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 44s\n"
     ]
    }
   ],
   "source": [
    "#import all you need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "\n",
    "#read the csv files and put them in pandas dataframes \n",
    "nyt_uniques=pd.read_csv('C:/Users/aditi/OneDrive/Desktop/Media_Bias/20180516-20180621_nyt_unique.csv')\n",
    "reuters_uniques=pd.read_csv('C:/Users/aditi/OneDrive/Desktop/Media_Bias/20180516-20180621_reuters_unique.csv')\n",
    "\n",
    "#define the stemmer you will be using in the functions below \n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "#define a funciton that both tokenizes and stems a given word. this will help us with the tfidf calculations\n",
    "def token_stem(text):\n",
    "    # tokenize by sentence and word. this way you ensure you get rid of punctuations\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # use the regex library to search only for items that contain letters. this will enable you to eliminate punctuation\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    stems = [stemmer.stem(t) for t in tokens_with_letters] #\"stems\" part\n",
    "    return stems\n",
    "\n",
    "#define a function that only tokenizes a given word. this won't help us with calculations, but will help us if \n",
    "#we ever need to lookup a stemmed word and see what the original version was. redundant mostly:) \n",
    "def token(text): #difference between this one and the one above is the \"stems\" part\n",
    "    #same as above\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens_with_letters = []\n",
    "    # same as above\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            tokens_with_letters.append(token)\n",
    "    return tokens_with_letters\n",
    "\n",
    "\n",
    "\n",
    "#put the articles into lists \n",
    "reuters_article_list=reuters_uniques['article'].tolist()\n",
    "nyt_article_list=nyt_uniques['article'].tolist()\n",
    "\n",
    "#get the stopwords. you will be eliminating them\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "#define a vectorizer that will help you put the words in the articles in vectors corresponding to these articles \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, max_features=200000, stop_words='english',use_idf=True, norm='l2',tokenizer=token_stem, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "#create a big list that is the union of the nyt articles and the reuters articles. you need to vectorize them together \n",
    "nyt_article_list.extend(reuters_article_list)\n",
    "\n",
    "\n",
    "%time tfidf_matrix = vectorizer.fit_transform(nyt_article_list) #fit the vectorizer into the big list of articles\n",
    "\n",
    "nyt_tfidf=tfidf_matrix[0:3406,:]\n",
    "reuters_tfidf=tfidf_matrix[3406:,:]\n",
    "\n",
    "#separate the big tfidf back into nyt_tfidf and reuters_tfidf. also put them in arrays because scipy.spatial.distance\n",
    "#can only work with arrays. \n",
    "nyt_tfidf_array_test=nyt_tfidf[0:10,:].toarray()\n",
    "reuters_tfidf_array_test=reuters_tfidf[0:100,:].toarray()\n",
    "\n",
    "\n",
    "#calculate distances between nyt - reuters articles. note this takes a lot of time! \n",
    "#output of this will be a matrix where each row corresponds to a reuters article and each column correspods to a NYT \n",
    "#article and each entry corresponds to the distance between the two articles \n",
    "import scipy.spatial.distance\n",
    "distances=scipy.spatial.distance.cdist(reuters_tfidf_array_test,nyt_tfidf_array_test,'cosine')\n",
    "\n",
    "\n",
    "\n",
    "#next chunk of code if basically putting the whole thing into the csv format that we have for the NYT hero articles and \n",
    "#the reuters articles. cleaning and indexing and all. \n",
    "our_indices=np.where(distances <=1 )\n",
    "reuters_indices=our_indices[0].tolist()\n",
    "nyt_indices=our_indices[1].tolist()\n",
    "our_values=pd.Series(distances[np.where(distances <=1)])\n",
    "distances_df = pd.concat([reuters_uniques['article'][reuters_indices].reset_index(),\n",
    "                          reuters_uniques['date'][reuters_indices].reset_index(),\n",
    "                          reuters_uniques['keywords'][reuters_indices].reset_index(),\n",
    "                          nyt_uniques['article'][nyt_indices].reset_index(),\n",
    "                          nyt_uniques['date'][nyt_indices].reset_index(),\n",
    "                          nyt_uniques['keywords'][nyt_indices].reset_index(),our_values]\n",
    "                         , axis=1)\n",
    "distances_df.columns=['index_reuters','article_reuters','index_reuters_1','date_reuters','index_reuters_2',\n",
    "                     'keywords_reuters','index_nyt','article_nyt','index_nyt_1','date_nyt','index_nyt_2','keywords_nyt',\n",
    "                     'distance']\n",
    "distances_df=distances_df[['index_reuters','article_reuters','date_reuters','keywords_reuters','index_nyt',\n",
    "                         'article_nyt','date_nyt','keywords_nyt','distance']]\n",
    "distances_df.to_csv('distances_nyt_Reuters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
